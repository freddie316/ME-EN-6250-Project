### How data was collected:
Our data was obtained by using the BeautifulSoup module to scrape the data from the HTML code of two websites: WorldOMeter and the WHO's covid dashboard. For WorldOMeter, the website HTML code was obtained using the requests module. The HTML code was then parsed by BeautifulSoup. The parsed data was searched for specific table entries which corresponded to "today", "yesterday", and "two days ago". Those tables were then looped through row by row, and column data is extracted for the specified country.
For the WHO's covid dashboard, it was a bit more tricky. Unlike WorldOMeter, the WHO's website is dynamic, and it's HTML code updates as things are clicked on the dashboard's menu. We used selenium to open a chrome page of the WHO's website, and first manipulate the website so that the desired HTML code could then be scraped. The actual scraping is much the same as for WorldOMeter. Beautifulsoup was used to parse the HTML code and locate the table by its role tag. The table was then looped through row by row. The releveant country's data is extracted just as before. 

### How the JSON is organized:
The code is currently set up to produce a unique JSON for each country, and each website. For example, requesting USA and UK data from WorldOMeter creates two JSONs: USA-WOM and UK-WOM, but then if USA and UK were requested from WHO, two new JSONs would be created for USA-WHO and UK-WHO. If a JSON already exists for a country from a website, the code will concatenate new data onto the existing JSON. Each JSON contains a pandas dataframe for the relevant data for a country from one of the websites. Here's an example of the dataframe:  
{"United States of America":{"0":"12\/02\/22"},"Total Deaths":{"0":"1,071,245"},"New Deaths":{"0":"1,488"},"Deaths\/1M pop":{"0":3236.4},"New Deaths\/1M pop":{"0":null}}  
The structure is very similar to a python dictionary. Each spot in the dataframe is set up like so: "column name" : {"row num":value}. Null values inside the dataframe correspond to datapoints that were either missing, or not provided by a website during the scraping.
